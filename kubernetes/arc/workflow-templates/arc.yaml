apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: arc
spec:

  templates:
  - name: createId
    script:
      image: triplai/arc:arc_3.2.0_spark_3.0.0_scala_2.12_hadoop_3.2.0_1.0.0
      command: [python3]
      source: |
        import uuid
        print(uuid.uuid4())

  - name: arcClient
    inputs:
      # override defaults here
      parameters:
      - name: image
        value: triplai/arc:arc_3.2.0_spark_3.0.0_scala_2.12_hadoop_3.2.0_1.0.0
      - name: driverCores
        value: 1
      - name: driverMemory
        value: 1G
      - name: executorInstances
        value: 1
      - name: executorCores
        value: 1
      - name: executorMemory
        value: 1G
      - name: pullPolicy
        value: Always
      - name: sparkConf
        value: ""
      - name: configUri
      - name: tags
        value: ""
      - name: parameters
        value: ""
    dag:
      tasks:
      - name: executionId
        template: createId
        continueOn:
          failed: false
      - name: createNamespacedService
        dependencies: [executionId]
        template: createNamespacedService
        arguments:
          parameters:
          - name: image
            value: "{{inputs.parameters.image}}"
          - name: executionId
            value: "{{tasks.executionId.outputs.result}}"
        continueOn:
          failed: false
      - name: arc
        dependencies: [createNamespacedService]
        template: arc
        arguments:
          parameters:
          - name: image
            value: "{{inputs.parameters.image}}"
          - name: executionId
            value: "{{tasks.executionId.outputs.result}}"
          - name: driverCores
            value: "{{inputs.parameters.driverCores}}"
          - name: driverMemory
            value: "{{inputs.parameters.driverMemory}}"
          - name: executorInstances
            value: "{{inputs.parameters.executorInstances}}"
          - name: executorCores
            value: "{{inputs.parameters.executorCores}}"
          - name: executorMemory
            value: "{{inputs.parameters.executorMemory}}"
          - name: pullPolicy
            value: "{{inputs.parameters.pullPolicy}}"
          - name: sparkConf
            value: "{{inputs.parameters.sparkConf}}"
          - name: configUri
            value: "{{inputs.parameters.configUri}}"
          - name: tags
            value: "{{inputs.parameters.tags}}"
          - name: parameters
            value: "{{inputs.parameters.parameters}}"
        continueOn:
          failed: true
      - name: deleteNamespacedService
        dependencies: [arc]
        template: deleteNamespacedService
        arguments:
          parameters:
          - name: image
            value: "{{inputs.parameters.image}}"
          - name: executionId
            value: "{{tasks.executionId.outputs.result}}"
          - name: exitCode
            value: "{{tasks.arc.exitCode}}"
        continueOn:
          failed: false

  - name: createNamespacedService
    inputs:
      parameters:
      - name: image
      - name: executionId
    metadata:
      labels:
        workflowId: "{{workflow.uid}}"
        executionId: "{{inputs.parameters.executionId}}"
    script:
      image: "{{inputs.parameters.image}}"
      command: ["/usr/bin/python3"]
      source: |
        # this script takes an input executionId and namespace and creates a headless service
        # with hostname 'arc-{executionId}-driver.{namespace}.svc.cluster.local'
        # this allows the executors to communicate to the driver
        # see https://kubernetes.io/docs/concepts/services-networking/service/#headless-services

        import sys
        import socket
        import time
        from kubernetes import client, config

        # bind input variables
        workflowId = '{{workflow.uid}}'
        executionId = '{{inputs.parameters.executionId}}'
        namespace = '{{workflow.namespace}}'

        # load injected service account details
        config.load_incluster_config()
        api_instance = client.CoreV1Api()

        # create meta
        meta = client.V1ObjectMeta()
        meta.name=f'arc-{executionId}-driver'
        meta.workflowId=workflowId
        meta.executionId=executionId

        # create spec
        spec = client.V1ServiceSpec()
        spec.cluster_ip = 'None'
        spec.selector = {
          'driverId': executionId
        }

        # expose ports
        driver_port = client.V1ServicePort(protocol='TCP', port=7078, target_port=7078, name='spark-driver-port')
        blockmanager_port = client.V1ServicePort(protocol='TCP', port=7079, target_port=7079, name='spark-blockmanager-port')
        spec.ports = [driver_port, blockmanager_port]

        # create service
        service = client.V1Service()
        service.api_version = 'v1'
        service.kind = 'Service'
        service.type = 'ClusterIP'
        service.spec = spec
        service.metadata = meta

        api_response = api_instance.create_namespaced_service(namespace=namespace, body=service)
        print(api_response)

  - name: arc
    inputs:
      parameters:
      - name: image
      - name: executionId
      - name: driverCores
      - name: driverMemory
      - name: executorInstances
      - name: executorCores
      - name: executorMemory
      - name: pullPolicy
      - name: sparkConf
      - name: configUri
      - name: tags
      - name: parameters
    metadata:
      labels:
        hostname: "arc-{{inputs.parameters.executionId}}-driver"
        driverId: "{{inputs.parameters.executionId}}"
        workflowId: "{{workflow.uid}}"
        executionId: "{{inputs.parameters.executionId}}"
    script:
      image: "{{inputs.parameters.image}}"
      env:
      - name: ETL_CONF_ENV
        value: "{{workflow.namespace}}"
      command: ["/bin/sh"]
      source: |
        # verbose logging
        set -ex

        # print current hostname
        hostname -I

        # wait for service to become available
        # if it does not after 30 seconds the spark-submit will fail anyway
        for i in {1..30}; do host arc-{{inputs.parameters.executionId}}-driver.{{workflow.namespace}}.svc.cluster.local && break || sleep 1; done

        # submit job
        bin/spark-submit \
        --master k8s://kubernetes.default.svc:443 \
        --deploy-mode client \
        --class ai.tripl.arc.ARC \
        --name arc \
        --conf spark.authenticate=true \
        --conf spark.blockManager.port=7079 \
        --conf spark.driver.extraJavaOptions="-XX:+UseG1GC" \
        --conf spark.driver.host=arc-{{inputs.parameters.executionId}}-driver.{{workflow.namespace}}.svc.cluster.local  \
        --conf spark.driver.memory={{inputs.parameters.driverMemory}} \
        --conf spark.driver.pod.name=$(hostname) \
        --conf spark.driver.port=7078 \
        --conf spark.executor.cores={{inputs.parameters.executorCores}} \
        --conf spark.executor.extraJavaOptions="-XX:+UseG1GC" \
        --conf spark.executor.instances={{inputs.parameters.executorInstances}} \
        --conf spark.executor.memory={{inputs.parameters.executorMemory}} \
        --conf spark.io.encryption.enabled=true \
        --conf spark.kubernetes.authenticate.caCertFile=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt \
        --conf spark.kubernetes.authenticate.driver.serviceAccountName={{workflow.serviceAccountName}} \
        --conf spark.kubernetes.authenticate.oauthTokenFile=/var/run/secrets/kubernetes.io/serviceaccount/token \
        --conf spark.kubernetes.container.image.pullPolicy={{inputs.parameters.pullPolicy}} \
        --conf spark.kubernetes.container.image={{inputs.parameters.image}} \
        --conf spark.kubernetes.driver.limit.cores={{inputs.parameters.driverCores}} \
        --conf spark.kubernetes.driver.pod.name=$(hostname) \
        --conf spark.kubernetes.executor.label.executionId={{inputs.parameters.executionId}} \
        --conf spark.kubernetes.executor.label.workflowId={{workflow.uid}} \
        --conf spark.kubernetes.executor.limit.cores={{inputs.parameters.executorCores}} \
        --conf spark.kubernetes.executor.podNamePrefix=$(hostname)-spark \
        --conf spark.kubernetes.executor.request.cores={{inputs.parameters.executorCores}} \
        --conf spark.kubernetes.local.dirs.tmpfs=false \
        --conf spark.kubernetes.namespace={{workflow.namespace}} \
        --conf spark.network.crypto.enabled=true \
        --conf spark.ui.enabled=true \
        --conf spark.sql.cbo.enabled=false \
        --conf spark.sql.adaptive.enabled=false \
        {{inputs.parameters.sparkConf}} \
        local:///opt/spark/jars/arc.jar \
        --etl.config.uri={{inputs.parameters.configUri}} \
        --etl.config.tags="workflowId={{workflow.uid}} executionId={{inputs.parameters.executionId}} serviceAccount={{workflow.serviceAccountName}} namespace={{workflow.namespace}} {{inputs.parameters.tags}}" \
        {{inputs.parameters.parameters}}

  - name: deleteNamespacedService
    inputs:
      parameters:
      - name: image
      - name: executionId
      - name: exitCode
    metadata:
      labels:
        workflowId: "{{workflow.uid}}"
        executionId: "{{inputs.parameters.executionId}}"
    script:
      image: "{{inputs.parameters.image}}"
      command: ["/usr/bin/python3"]
      source: |
        # this script takes an input executionId and namespace and removes an exposed service
        # it is the equivalent to a 'finally' in a 'try-catch-finally' pattern as it will be executed regardless
        # of the outcome of the arc stage due to the 'continueOn: failed: true' condition.

        import sys
        from kubernetes import client, config

        # bind input variables
        executionId = '{{inputs.parameters.executionId}}'
        namespace = '{{workflow.namespace}}'
        exitCode = int('{{inputs.parameters.exitCode}}')

        # load injected service account details
        config.load_incluster_config()
        api_instance = client.CoreV1Api()

        api_response = api_instance.delete_namespaced_service(f'arc-{executionId}-driver', namespace)
        print(api_response)

        # return the exitCode from the arc stage as the final job outcome
        sys.exit(exitCode)
